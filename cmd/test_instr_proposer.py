#!/usr/bin/env python3
"""
Instruction Candidate Generator and Cache Builder.

Generates instruction candidates for all modules in the QA program,
exactly like MIPROOptimizer does during optimization, and caches them
for faster optimization runs.

Recommended usage from project root:
    python -m cmd.test_instr_proposer [--use-cached-demos]

Also works when run directly as a script.
"""

import argparse
import logging
import os
import sys
import pprint  # Added for pretty-printing candidate sets

# Ensure project root is on sys.path when running as a script
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from backend import LMBackend
from QADataset import QADataset
from programs import QAProgram
from helpers import InstructionProposer
from logging_config import setup_logging
from config import apply_tier
from cache.candidate_cache import (
    save_instruction_candidates,
    load_demo_candidates,
    cache_exists,
)


logger = logging.getLogger(__name__)


def main():
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Generate instruction candidates for optimization (like MIPROOptimizer)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This script generates instruction candidates exactly as MIPROOptimizer does during
optimization, then caches them for faster optimization runs.

Examples:
  python -m cmd.test_instr_proposer                    # Generate with empty demos
  python -m cmd.test_instr_proposer --use-cached-demos # Use cached demos (recommended)
  python -m cmd.test_instr_proposer --check-cache      # Check cache status
        """,
    )
    parser.add_argument(
        "--use-cached-demos",
        action="store_true",
        help="Load demo candidates from cache (generated by test_bootstrapper.py)",
    )
    parser.add_argument(
        "--check-cache",
        action="store_true",
        help="Check if demo cache exists and exit",
    )
    args = parser.parse_args()

    # Handle --check-cache
    if args.check_cache:
        cache_status = cache_exists()
        print("Demo Cache Status:")
        if cache_status['demos']:
            print("  ✓ Demo candidates cache EXISTS")
            print("\nTo use cached demos, run with: --use-cached-demos")
        else:
            print("  ✗ Demo candidates cache NOT FOUND")
            print("\nTo generate demo cache, run:")
            print("  python -m cmd.test_bootstrapper")
        return

    # Apply LIGHT tier for fast testing (BEFORE loading dataset)
    tier_config = apply_tier("light")

    setup_logging(
        level=logging.INFO,
        fmt="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    )

    logger.info("=== InstructionProposer standalone test (LIGHT tier) ===")
    logger.info("Using MAX_EXAMPLES=%d", tier_config.max_examples)
    if args.use_cached_demos:
        logger.info("Will attempt to load demo candidates from cache")

    # 1. Load dataset (will use tier-configured MAX_EXAMPLES)
    logger.info("Loading dataset...")
    dataset = QADataset()
    dataset.load()
    train_data = dataset.get_split("train")
    if not train_data:
        logger.error("No training data available; cannot generate instructions.")
        return
    logger.info("  Loaded %d training examples", len(train_data))

    # 2. Initialize QAProgram and shared LM backend
    logger.info("Initializing QAProgram...")
    lm = LMBackend()
    program = QAProgram(backend=lm)
    module_names = sorted(program.get_module_names())  # Sort for consistent predictor indexing
    logger.info("  Modules: %s", module_names)

    # 3. Initialize InstructionProposer with full train_data (like MIPROOptimizer does)
    logger.info("Initializing InstructionProposer (generating dataset/program summaries)...")
    proposer = InstructionProposer(
        lm=lm, train_examples=train_data, program=program
    )
    logger.info("  ✓ Summaries generated")

    # 4. Load or prepare demo candidates
    from config import TASK_DESCRIPTION
    task_desc = TASK_DESCRIPTION
    logger.info("Loading demo candidates...")
    if args.use_cached_demos:
        logger.info("  Attempting to load from cache...")
        demo_candidates = load_demo_candidates()
        if demo_candidates is None:
            logger.warning(
                "  Cache not found. Using empty demos.\n"
                "  Run 'python -m cmd.test_bootstrapper' to generate demo cache."
            )
            demo_candidates = {idx: [] for idx in range(len(module_names))}
        else:
            logger.info("  Loaded demo candidates from cache")
            for predictor_idx, candidate_sets in demo_candidates.items():
                num_demos = len(candidate_sets[0]) if candidate_sets else 0
                logger.info(
                    "    Predictor %d: %d candidate sets × %d demos each",
                    predictor_idx,
                    len(candidate_sets),
                    num_demos,
                )
    else:
        logger.info("  Using empty demos (no --use-cached-demos flag)")
        demo_candidates = {idx: [] for idx in range(len(module_names))}
    
    # 5. Generate instruction candidates (exactly like MIPROOptimizer does)
    logger.info("Generating instruction candidates...")
    logger.info("  Generating %d candidates per module", tier_config.n_instruction_candidates)
    
    all_candidates = proposer.propose_for_all_modules(
        program=program,
        task_desc=task_desc,
        bootstrapped_demos=demo_candidates,
        n_candidates=tier_config.n_instruction_candidates,
        program_aware=True,
        module_names=module_names,
    )

    # Log summary
    logger.info("")
    logger.info("Generated instruction candidates:")
    for predictor_idx, candidates in all_candidates.items():
        module_name = module_names[predictor_idx]
        logger.info(
            "  Predictor %d (%s): %d candidates (1 original + %d proposed)",
            predictor_idx,
            module_name,
            len(candidates),
            len(candidates) - 1,
        )

    # Show sample instructions (first 2 per module)
    logger.instr("\n" + "=" * 60)
    logger.instr("Sample Instructions (showing first 2 per module)")
    logger.instr("=" * 60)
    for predictor_idx, instruction_list in all_candidates.items():
        module_name = module_names[predictor_idx]
        logger.instr(f"\n[Predictor {predictor_idx}: {module_name}]")
        for instr_idx, instruction in enumerate(instruction_list[:2]):
            label = "original" if instr_idx == 0 else f"candidate_{instr_idx}"
            logger.instr(f"  [{label}]")
            # Show first 80 chars only for readability
            first_line = instruction.strip().split('\n')[0]
            if len(first_line) > 80:
                first_line = first_line[:80] + "..."
            logger.instr(f"    {first_line}")
    logger.instr("")

    # Save instruction candidates to cache
    logger.info("Saving instruction candidates to cache...")
    metadata = {
        "tier": "light",
        "num_modules": len(module_names),
        "module_names": module_names,
        "num_train_examples": len(train_data),
        "task_desc": task_desc,
        "used_cached_demos": args.use_cached_demos,
        "n_candidates_per_module": tier_config.n_instruction_candidates,
    }
    cache_path = save_instruction_candidates(all_candidates, metadata=metadata)
    logger.info("  ✓ Saved to: %s", cache_path)
    logger.info("")
    logger.info("=" * 60)
    logger.info("✓ Instruction candidates ready for optimization!")
    logger.info("=" * 60)
    logger.info("Next step: python main.py --tier light --use-cache")


if __name__ == "__main__":
    main()
